import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay
from matplotlib.backends.backend_pdf import PdfPages
import os

# =================== CONFIGURA√á√ïES ===================

# Caminho dos arquivos (modifique conforme necess√°rio)
csv_resultados = r"C:\Users\DettCloud2\Downloads\tcc\results_lote\relatorio_lote.csv"
csv_metadata = r"C:\Users\DettCloud2\Downloads\tcc\ham10000\HAM10000_metadata.csv"
output_folder = r"C:\Users\DettCloud2\Downloads\tcc\results_lote"

os.makedirs(output_folder, exist_ok=True)

# =================== CARREGAR DADOS ===================

def carregar_dados(csv_resultados, csv_metadata):
    """Carregar os dados dos arquivos CSV."""
    resultados = pd.read_csv(csv_resultados)
    metadata = pd.read_csv(csv_metadata)

    # Remover extens√£o '.jpg' das imagens no resultados
    resultados['imagem'] = resultados['imagem'].str.replace('.jpg', '', regex=False)

    # Merge entre resultados e metadados
    df = pd.merge(resultados, metadata[['image_id', 'dx']], left_on='imagem', right_on='image_id')
    
    # Mapeamento para diagn√≥stico real (SUSPEITA ou BENIGNA)
    mapa = {
        'akiec': 'SUSPEITA',
        'bcc': 'SUSPEITA',
        'mel': 'SUSPEITA',
        'bkl': 'PROVAVELMENTE BENIGNA',
        'df': 'PROVAVELMENTE BENIGNA',
        'nv': 'PROVAVELMENTE BENIGNA',
        'vasc': 'PROVAVELMENTE BENIGNA'
    }
    df['diagnostico_real'] = df['dx'].map(mapa)

    return df

# =================== C√ÅLCULO DE M√âTRICAS ===================

def calcular_metricas(y_true, y_pred):
    """Calcular as m√©tricas de desempenho do modelo."""
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, pos_label="SUSPEITA")
    rec = recall_score(y_true, y_pred, pos_label="SUSPEITA")
    f1 = f1_score(y_true, y_pred, pos_label="SUSPEITA")
    
    return acc, prec, rec, f1

# =================== GERAR MATRIZ DE CONFUS√ÉO ===================

def gerar_matriz_confusao(y_true, y_pred):
    """Gerar e exibir a matriz de confus√£o."""
    cm = confusion_matrix(y_true, y_pred, labels=["SUSPEITA", "PROVAVELMENTE BENIGNA"])
    
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["SUSPEITA", "PROVAVELMENTE BENIGNA"])
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Matriz de Confus√£o")
    plt.show()

    return cm

# =================== GERAR RELAT√ìRIO EM PDF ===================

def gerar_relatorio_pdf(acc, prec, rec, f1, cm, output_folder):
    """Gerar um relat√≥rio em PDF com as m√©tricas e matriz de confus√£o."""
    pdf_path = os.path.join(output_folder, "relatorio_desempenho.pdf")
    pdf = PdfPages(pdf_path)

    # P√°gina de M√©tricas
    plt.figure(figsize=(8.5, 11))
    plt.axis("off")
    plt.title("Relat√≥rio de Desempenho do Modelo", fontsize=16, pad=20, weight="bold")
    
    texto = (
        f"üìä Avalia√ß√£o Quantitativa\n\n"
        f"- Acur√°cia: {acc*100:.2f}%\n"
        f"- Precis√£o (SUSPEITA): {prec*100:.2f}%\n"
        f"- Recall/Sensibilidade (SUSPEITA): {rec*100:.2f}%\n"
        f"- F1-Score: {f1*100:.2f}%\n\n"
        f"üîç Interpreta√ß√£o:\n"
        f"- Acur√°cia geral reflete a taxa de acertos do modelo.\n"
        f"- Precis√£o indica quantos dos classificados como SUSPEITA realmente s√£o.\n"
        f"- Recall mede a capacidade de detectar todas as les√µes SUSPEITAS.\n"
        f"- F1-Score √© o equil√≠brio entre precis√£o e recall.\n\n"
        f"üìù Observa√ß√µes:\n"
        f"- A matriz de confus√£o mostra erros e acertos claramente.\n"
        f"- O recall alto √© importante para n√£o deixar les√µes SUSPEITAS passarem.\n"
    )
    plt.text(0, 1, texto, fontsize=12, va="top")
    
    pdf.savefig()
    plt.close()

    # P√°gina da Matriz de Confus√£o
    fig, ax = plt.subplots(figsize=(8, 8))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["SUSPEITA", "PROVAVELMENTE BENIGNA"])
    disp.plot(cmap="Blues", values_format="d", ax=ax, colorbar=False)

    plt.title("Matriz de Confus√£o", fontsize=16, weight="bold")
    plt.grid(False)
    plt.xlabel("Classe Predita", fontsize=12)
    plt.ylabel("Classe Real", fontsize=12)
    plt.subplots_adjust(left=0.436, bottom=0.367, right=0.998, top=0.938, wspace=0.2, hspace=0.2)
    plt.tight_layout()
    pdf.savefig()
    plt.close()

    # Fechar PDF
    pdf.close()

    print(f"‚úÖ Relat√≥rio PDF salvo em: {pdf_path}")

# =================== GERAR CSV DE COMPARA√á√ÉO ===================

def salvar_comparacao(df, output_folder):
    """Salvar o CSV de compara√ß√£o entre diagn√≥stico real e classifica√ß√£o do modelo."""
    output_csv = os.path.join(output_folder, "comparacao.csv")
    df[['imagem', 'diagnostico_real', 'classificacao']].to_csv(output_csv, index=False)
    print(f"‚úÖ CSV de compara√ß√£o salvo em: {output_csv}")

# =================== EXECU√á√ÉO PRINCIPAL ===================

def main():
    # Carregar dados
    df = carregar_dados(csv_resultados, csv_metadata)

    # Labels reais e preditos
    y_true = df['diagnostico_real']
    y_pred = df['classificacao']

    # Calcular m√©tricas
    acc, prec, rec, f1 = calcular_metricas(y_true, y_pred)

    # Mostrar no terminal
    print("üìä Avalia√ß√£o do Desempenho:")
    print(f"Acur√°cia: {acc*100:.2f}%")
    print(f"Precis√£o (SUSPEITA): {prec*100:.2f}%")
    print(f"Recall/Sensibilidade (SUSPEITA): {rec*100:.2f}%")
    print(f"F1-Score: {f1*100:.2f}%")

    # Gerar matriz de confus√£o
    cm = gerar_matriz_confusao(y_true, y_pred)

    # Gerar relat√≥rio PDF
    gerar_relatorio_pdf(acc, prec, rec, f1, cm, output_folder)

    # Salvar compara√ß√£o em CSV
    salvar_comparacao(df, output_folder)

if __name__ == "__main__":
    main()
